{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxYigmxa27gk"
      },
      "outputs": [],
      "source": [
        "# ðŸ“¦ Step 1: Install all required packages\n",
        "!pip install -q unsloth\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q langchain\n",
        "!pip install -q accelerate\n",
        "!pip install -q transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/unslothai/unsloth.git\n"
      ],
      "metadata": {
        "id": "x8LN5Eca48u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ðŸ§  Choose Unsloth 4-bit quantized model\n",
        "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "# ðŸ”„ Load model in 4-bit for low memory usage\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    load_in_4bit = True,\n",
        "    device_map = \"auto\"  # GPU use karega agar available ho\n",
        ")\n",
        "\n",
        "# âœ… Make model fast for inference\n",
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {
        "id": "U2UQ3Uyz4fXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86610468"
      },
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bbd473d"
      },
      "source": [
        "!pip install -q unsloth_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Poppler (needed for PDF rendering)\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "# Required Python packages\n",
        "!pip install pdf2image\n",
        "!pip install unstructured\n",
        "!pip install \"unstructured[local-inference]\"\n"
      ],
      "metadata": {
        "id": "Myy4kYdN9wzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Attention.pdf\")\n",
        "documents = loader.load()\n",
        "print(f\"Total documents loaded: {len(documents)}\")\n"
      ],
      "metadata": {
        "id": "E0rDBHv19_8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunking config\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Split your PDF documents into chunks\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "16mVV0wg-L2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Use a compact, fast model (for Colab compatibility)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "6HLvPvyb-QMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Index the chunks\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "\n",
        "# Save it locally (optional)\n",
        "vectorstore.save_local(\"rag_faiss_index\")\n"
      ],
      "metadata": {
        "id": "ptVMINfh-Yj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# âœ… Allow loading trusted pickle file\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"rag_faiss_index\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "ggRcOHr9-j5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
        "\n",
        "# âœ… Prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the following context to answer the question **clearly and concisely**.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "def generate_response(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Optional cleanup: remove duplicate lines if model repeats\n",
        "    lines = decoded.splitlines()\n",
        "    unique_lines = list(dict.fromkeys(lines))  # Removes duplicates while preserving order\n",
        "    return \"\\n\".join(unique_lines)\n",
        "\n",
        "# New RAG chain with better prompt\n",
        "rag_chain = (\n",
        "    RunnableMap({\n",
        "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "    })\n",
        "    | RunnableLambda(lambda x: prompt.format(**x))  # use improved prompt\n",
        "    | RunnableLambda(generate_response)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "query = \"What is self-attention mechanism in transformers?\"\n",
        "response = rag_chain.invoke({\"question\": query})\n",
        "print(\"ðŸ¤– Answer:\", response)\n",
        "\n",
        "\n",
        "# âœ… Ask a question\n",
        "query = \"What is self-attention mechanism in transformers?\"\n",
        "response = rag_chain.invoke({\"question\": query})\n",
        "print(\"ðŸ¤– Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACzrUs0d_F9G",
        "outputId": "f9657ccb-2d88-4c5e-b2bd-94b3760fe35e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Answer: Human: \n",
            "Use the following context to answer the question **clearly and concisely**.\n",
            "\n",
            "Context:\n",
            "[Document(id='286ea722-a4c4-4f75-a0a7-a472772932c9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'), Document(id='792b2b2d-deeb-46a7-abd6-915fd22f32c8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'), Document(id='e515d88f-3628-49ba-a253-281023a357b3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been')]\n",
            "Question:\n",
            "What is self-attention mechanism in transformers?\n",
            "Answer:\n",
            "The self-attention mechanism in transformers is a mechanism that relates different positions of a single sequence in order to compute a representation of the sequence.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¤– Answer: Human: \n",
            "Use the following context to answer the question **clearly and concisely**.\n",
            "\n",
            "Context:\n",
            "[Document(id='286ea722-a4c4-4f75-a0a7-a472772932c9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'), Document(id='792b2b2d-deeb-46a7-abd6-915fd22f32c8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'), Document(id='e515d88f-3628-49ba-a253-281023a357b3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '','moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5','subject': '', 'title': '', 'trapped': '/False','source': 'Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been')]\n",
            "Question:\n",
            "What is self-attention mechanism in transformers?\n",
            "Answer:\n",
            "The self-attention mechanism is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used in many NLP tasks such as machine translation, question answering, and language modeling.\n"
          ]
        }
      ]
    }
  ]
}